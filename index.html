<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-VPZ41Q777R"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-VPZ41Q777R');
	</script>
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Xiaofeng&#39;s Blog">
    <meta name="author" content="Xiaofeng Gao">

    <title>Xiaofeng Gao</title>

    <!-- Bootstrap Core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- Main CSS -->
    <link href="./css/main.css" rel="stylesheet">

    <!-- Font Awesome CSS -->
    <link rel="stylesheet" href="./css/font-awesome.min.css">

    <!-- icon -->
    <link rel="shortcut icon" href="imgs/xfgao.jpg" type="image/x-icon">
    <link rel="icon" href="imgs/xfgao.jpg" type="image/x-icon">

    <!-- Google Fonts -->
    <link rel="stylesheet" href="./css/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top navbar-inverse bg-inverse top-nav-collapse" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="http://xfgao.github.io/#page-top">XIAOFENG GAO</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden active">
                        <a class="page-scroll" href="http://xfgao.github.io/#page-top"></a>
                    </li>
                    <li class="">
                        <a class="page-scroll" href="http://xfgao.github.io/#about">ABOUT</a>
                    </li>
<!--                     <li>
                        <a class="page-scroll" href="#projects">Projects</a>
                    </li> -->
                    <li class="">
                        <a class="page-scroll" href="http://xfgao.github.io/#publications">PUBLICATIONS</a>
                    </li>
                    <!-- <li>
                        <a class="page-scroll" href="#awards">Awards</a>
                    </li> -->
                    <li>
                        <a class="page-scroll" href="https://xfgao.github.io/CV_XiaofengGao_July2024.pdf">CV</a>
                    </li>
                </ul>
<!--                 <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="mailto:xfgao@ucla.edu" class="navbar-brand"><i class="fa fa-envelope"></i></a>
                    </li>
                    <li>
                        <a href="https://github.com/xfgao/" class="navbar-brand"><i class="fa fa-github"></i></a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=AjTfCjEAAAAJ&hl=en&authuser=1" class="navbar-brand"><i class="fa fa-graduation-cap"></i></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/xiaofeng-gao-09b966115/" class="navbar-brand"><i class="fa fa-linkedin"></i></a>
                    </li>
                    <li>
                        <a href="https://www.facebook.com/profile.php?id=100010717656664" class="navbar-brand"><i class="fa fa-facebook"></i></a>
                    </li>

                </ul> -->

            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-xs-12 col-sm-6 col-md-5 col-lg-4">
                        <img src="./imgs/xfgao.jpg" class="img-responsive img-circle img-fluid" width="250" align="right">
                </div>
                <div class="col-xs-12 col-md-6 col-md-7 col-lg-8 personal-intro">
                    <div class="col-xs-12 col-sm-12 col-md-12">
                        <h2 class="text-uppercase">Xiaofeng Gao</h2>
                        <address>
                            1120 Enterprise Way<br>
                            Sunnyvale, CA, 94089
                        </address>
                        <span><strong>Email: </strong>xfgao at ucla dot edu</span>
                        <br>
                        <a href="https://scholar.google.com/citations?user=AjTfCjEAAAAJ&hl=en&authuser=1">[Google Scholar]</a>&nbsp;&nbsp;
						<a href="https://github.com/xfgao">[GitHub]</a>
                    </div>
					
                </div>
            </div>
        </div>
    </section>
    <hr>
    <!-- About Section -->
    <section id="about" class="about-section">
        <div class="container">
            <h1>About</h1>
            <div class="bio-info">
                <p> I'm an Applied Scientist at Amazon. My research lies in the intersection of Robotics, Computer Vision, Machine Learning and Cognitive Science, with a focus on developing <strong> cognitively inspired cooperative agents</strong>. I received my PhD in Statistics from <a href="https://www.ucla.edu/">University of California, Los Angeles</a> under the supervision of <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a>. 

                <p> During my PhD, I also worked closely with <a href="https://www.psych.ucla.edu/faculty-page/hongjing/">Prof. Hongjing Lu</a> (UCLA), <a href="https://viterbi.usc.edu/directory/faculty/Sukhatme/Gaurav">Prof. Gaurav Sukhatme</a> (USC & Amazon) and <a href="http://tshu.io">Prof. Tianmin Shu</a> (JHU). Before that, I obtained a bachelor degree of Electronic Engineering at <a href="http://www.fudan.edu.cn/en/"> Fudan University.</a></p>
            </div>

        </div>
    </section>
    <hr>

    <!-- News Section -->
    <section id="news" class="news-section">
        <div class="container">
        	<h1>News</h1>
            <div class="news">
                <p><strong>04/2024</strong>: We are hosting the <a href="https://sites.google.com/view/arnoldchallenge/">ARNOLD challenge</a> as a part of <a href="https://embodied-ai.org/">CVPR 2024 Embodied AI Workshop</a>. Welcome to participate!
                <p><strong>02/2024</strong>: Groundhog is accepted by CVPR. 
                <p><strong>09/2023</strong>: Alexa Arena is accepted by NeurIPS 2023 Datasets and Benchmarks track.
                <p><strong>08/2023</strong>: LEMMA is accepted by RA-L.
                <p><strong>07/2023</strong>: ARNOLD is accepted by ICCV 2023.
                <!-- <p><strong>06/2023</strong>: We organized the <a href="https://eval.ai/web/challenges/challenge-page/1859/overview">DialFRED</a> challenge in the <a href="https://embodied-ai.org/">CVPR 2023 Embodied AI workshop</a>. -->
                <!-- <p><strong>03/2023</strong>: We released Alexa Arena, a user-centric Embodied AI platform. -->
            	<!-- <p><strong>12/2022</strong>: ARNOLD is accepted as Spotlight by <a href="https://sites.google.com/view/langrob-corl22/">CORL 2022 Workshop on Language and Robot Learning</a>. -->
            	<!-- <p><strong>07/2022</strong>: Our paper on In-situ bidirectional human-robot value alignment is published on <a href="https://www.science.org/doi/10.1126/scirobotics.abm4183">Science Robotics</a>. -->
                <!-- <p><strong>07/2022</strong>: Our paper on Dialogue-Enabled Agents for Embodied Instruction Following is accepted by RA-L. -->
                <!-- <p><strong>06/2022</strong>: I defended my PhD dissertation.  -->
            	<!-- <p><strong>04/2022</strong>: One paper on the effects of AR-based human-machine interface on drivers' situational awareness has been accepted by IV 2022.  -->
            	<!-- <p><strong>02/2022</strong>: Our paper on <a href="https://xfgao.github.io/calib2022ral/">robot capability calibration</a> is covered by <a href="https://techxplore.com/news/2022-02-reachability-expressive-motion-algorithm-human-robot-collaboration.html">TechXplore</a>.  -->
            	<!-- <p><strong>01/2022</strong>: Our paper "Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration" has been accepted by IEEE Robotics and Automation Letters.  -->
            	<!-- <p><strong>05/2021</strong>: Our paper "Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration" has been accepted for a <strong>spotlight</strong> presentation at the ICRA Workshop on Social Intelligence in Humans and Robots. <a href="https://social-intelligence-human-ai.github.io/">[Link]</a> -->
                <!-- <p><strong>01/2021</strong>: We presented our paper "Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks" at IJCAI 2020 Workshop on XAI. <a href="https://sites.google.com/view/xai2020/home">[Link]</a> -->
                <!-- <p><strong>01/2021</strong>: I started my internship at Honda Research Institute USA <a href="https://usa.honda-ri.com/">[Link]</a>. I would be working on projects related to Human-Machine Interaction. </p> -->
                <!-- <p><strong>07/2020</strong>: One paper got accepted by RO-MAN 2020. <a href="http://ro-man2020.unina.it/">[Link] </a></p> -->
            	<!-- <p><strong>05/2019</strong>: One paper got accepted by ICML workshop on Reinforcement Learning for Real Life. <a href="https://sites.google.com/view/RL4RealLife">[Link] </a></p> -->
            	<!-- <p><strong>03/2019</strong>: VRKitchen was covered by TechXplore. <a href="https://techxplore.com/news/2019-03-vrkitchen-interactive-virtual-environment-ai.html">[Link] </a> </p> -->
            	<!-- <p><strong>03/2019</strong>: I was invited as a reviewer for IROS 2019.</p> -->
            	<!-- <p><strong>03/2019</strong>: We submitted a paper to arXiv as a <a href="https://arxiv.org/pdf/1903.05757.pdf">technical report for VRKitchen</a>.</p> -->
            	<!-- <p><strong>03/2019</strong>: I passed the Oral Qualifying Exam and advanced to candidancy!</p> -->
            	<!-- <p><strong>02/2019</strong>: I gave a poster presentation at 
            		<a href="https://sites.google.com/go.spawar.navy.mil/naml/home"> the Third Annual Workshop on Naval Applications of Machine Learning </a>.</p>
            	<p><strong>09/2018</strong>: I was TAing for "<i>Stats 10: Introduction to Statistical Reasoning</i>", Fall 2018.</p>
                <p><strong>09/2017</strong>: I started my Ph.D. studies at UCLA.</p>
                <p><strong>03/2017</strong>: Our ICRA 2017 work was covered by New Scientist. <a href="https://www.newscientist.com/article/2129162-robots-taught-to-work-alongside-humans-by-giving-high-fives/">[Link] </a> </p> -->
            </div>
        </div>
    </section>
    <hr></hr>


    <section id="publications" class="publications-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Publications</h1>
                        <div class="contents">
                            <ul class="list-group">
                            	(* indicates equal contribution)
                                <!-- TeamCraft -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/teamcraft.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft</h4>
                                        <p class="detail">
                                            Qian Long, Zhi Li, Ran Gong, Ying Nian Wu, Demetri Terzopoulos, <strong>Xiaofeng Gao</strong> <br>
                                            <em>arXiv Preprint</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#teamcraft-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/Arxiv2024_TeamCraft.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#teamcraft-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://teamcraft-bench.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="teamcraft-abs" class="collapse abstract">
                                        Collaboration is a cornerstone of society. In the real world, human teammates make use of multi-sensory data to tackle challenging tasks in ever-changing environments. It is essential for embodied agents collaborating in visually-rich environments replete with dynamic interactions to understand multi-modal observations and task specifications. To evaluate the performance of generalizable multi-modal collaborative agents, we present TeamCraft, a multi-modal multi-agent benchmark built on top of the open-world video game Minecraft. The benchmark features 55,000 task variants specified by multi-modal prompts, procedurally-generated expert demonstrations for imitation learning, and carefully designed protocols to evaluate model generalization capabilities. We also perform extensive analyses to better understand the limitations and strengths of existing approaches. Our results indicate that existing models continue to face significant challenges in generalizing to novel goals, scenes, and unseen numbers of agents. These findings underscore the need for further research in this area.
                                    </div>

                                    <div id="teamcraft-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{long2024teamcraft,
  title={TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft},
  author={Long, Qian and Li, Zhi and Gong, Ran and Wu, Ying Nian and Terzopoulos, Demetri and Gao, Xiaofeng},
  journal={arXiv preprint arXiv:2412.05255},
  year={2024}
}</pre>
                                    </div>  
                                </li>

                                <!-- MIDAS -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/midas.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning</h4>
                                        <p class="detail">
                                            Jiachen Li, Qiaozi Gao, Michael Johnston, <strong>Xiaofeng Gao</strong>, Xuehai He, Hangjie Shi, Suhaila Shakiah, Reza Ghanadan, William Yang Wang <br>
                                            <em> International Conference on Machine Learning (ICML), 2024</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#midas-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/ICML2024_MIDAS.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#midas-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://midas-icml.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="midas-abs" class="collapse abstract">
                                        Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models’ tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving vision signals with text descriptions. This type of task poses a major challenge to robots’ capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability.
                                    </div>

                                    <div id="midas-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@InProceedings{li2024mastering,
  title =    {Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning},
  author =       {Li, Jiachen and Gao, Qiaozi and Johnston, Michael and Gao, Xiaofeng and He, Xuehai and Shi, Hangjie and Shakiah, Suhaila and Ghanadan, Reza and Wang, William Yang},
  booktitle =    {Proceedings of the 41st International Conference on Machine Learning},
  pages =    {27822--27845},
  year =     {2024},
  editor =   {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume =   {235},
  series =   {Proceedings of Machine Learning Research},
  month =    {21--27 Jul},
  publisher =    {PMLR},
  pdf =      {https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24x/li24x.pdf},
  url =      {https://proceedings.mlr.press/v235/li24x.html},
}</pre>
                                    </div>  
                                </li>

                                <!-- Groundhog -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/groundhog.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</h4>
                                        <p class="detail">
                                            Yichi Zhang, 
                                            Ziqiao Ma, 
                                            <strong>Xiaofeng Gao</strong>,
                                            Suhaila Shakiah, 
                                            Qiaozi Gao, 
                                            Joyce Chai <br>
                                            <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#groundhog-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/CVPR2024_Groundhog.pdf">PDF</a>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/CVPR2024_Groundhog_Supp.pdf">Supp</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#groundhog-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://groundhog-mllm.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="groundhog-abs" class="collapse abstract">
                                        Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.
                                    </div>

                                    <div id="groundhog-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@InProceedings{zhang2024groundhog,
    author    = {Zhang, Yichi and Ma, Ziqiao and Gao, Xiaofeng and Shakiah, Suhaila and Gao, Qiaozi and Chai, Joyce},
    title     = {GROUNDHOG: Grounding Large Language Models to Holistic Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14227-14238}
}</pre>
                                    </div>  
                                </li>

                                <!-- LEMMA -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/lemma.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">LEMMA: Learning Language-Conditioned Multi-Robot Manipulation</h4>
                                        <p class="detail">
                                            Ran Gong,
                                            <strong>Xiaofeng Gao</strong>, 
                                            Qiaozi Gao,                                       
                                            Suhaila Shakiah,
                                            Govind Thattai,
                                            Gaurav S. Sukhatme <br>
                                            <em> IEEE Robotics and Automation Letters (RA-L), 2023</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#lemma-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/RAL2023_LEMMA.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#lemma-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://lemma-benchmark.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="lemma-abs" class="collapse abstract">
                                        Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.
                                    </div>

                                    <div id="lemma-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@ARTICLE{gong2023lemma,
  author={Gong, Ran and Gao, Xiaofeng and Gao, Qiaozi and Shakiah, Suhaila and Thattai, Govind and Sukhatme, Gaurav S.},
  journal={IEEE Robotics and Automation Letters}, 
  title={LEMMA: Learning Language-Conditioned Multi-Robot Manipulation}, 
  year={2023},
  volume={8},
  number={10},
  pages={6835-6842},
  keywords={Task analysis;Robots;Robot kinematics;Planning;Benchmark testing;Collaboration;Multitasking;Multi-robot systems;Data Sets for Robot Learning;Natural Dialog for HRI;Multi-Robot Systems},
  doi={10.1109/LRA.2023.3313058}}</pre>
                                    </div>  
                                </li>

                                <!-- Arena -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/arena_teaser.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Alexa Arena: A User-Centric Interactive Platform for Embodied AI</h4>
                                        <p class="detail">
                                            Qiaozi Gao*, Govind Thattai*, Suhaila Shakiah*, <strong>Xiaofeng Gao*</strong>, Shreyas Pansare, Vasu Sharma, Gaurav S. Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zhang, Lucy Hu, Karthika Arumugam, Shui Hu, Matthew Wen, Dinakar Venkateswar Guthy, Shunan Cadence Chung, Rohan Khanna, Osman Ipek, Leslie Ball, Kate Bland, Heather Rocker, Michael Johnston, Reza Ghanadan, Dilek Hakkani-Tur, Prem Natarajan <br>
                                            <em> Conference on Neural Information Processing Systems (NeurIPS), 2023</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#arena-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/Neurips2023_Arena.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#arena-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://github.com/amazon-science/alexa-arena">Code&Data</a>
                                        </div>
                                    </div>
                                    <div id="arena-abs" class="collapse abstract">
                                        We introduce Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research. Alexa Arena features multi-room layouts and an abundant of interactable objects. With user-friendly graphics and control mechanisms, the platform supports the development of gamified robotic tasks readily accessible to general human users, allowing high-efficiency data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled task completion benchmark with online human evaluations. We make Alexa Arena publicly available to facilitate research in building assistive conversational embodied agents.

                                    </div>

                                    <div id="arena-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@inproceedings{gao2023alexa,
 author = {Gao, Qiaozi and Thattai, Govind and Shakiah, Suhaila and Gao, Xiaofeng and Pansare, Shreyas and Sharma, Vasu and Sukhatme, Gaurav and Shi, Hangjie and Yang, Bofei and Zhang, Desheng and Hu, Lucy and Arumugam, Karthika and Hu, Shui and Wen, Matthew and Guthy, Dinakar and Chung, Shunan and Khanna, Rohan and Ipek, Osman and Ball, Leslie and Bland, Kate and Rocker, Heather and Johnston, Michael and Ghanadan, Reza and Hakkani-Tur, Dilek and Natarajan, Prem},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {19170--19194},
 publisher = {Curran Associates, Inc.},
 title = {Alexa Arena: A User-Centric Interactive Platform for Embodied AI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3d0758f0b95e19abc68c1c8070d36510-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}</pre>
                                    </div>  
                                </li>

                                <!-- Arnold -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/arnold.gif" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">ARNOLD: A Benchmark for Language-Grounded Task Learning with Continuous States in Realistic Scenes</h4>
                                        <p class="detail">
                                            <a href="https://nikepupu.github.io/">Ran Gong*</a>,
                                            <a href="https://huangjy-pku.github.io/">Jiangyong Huang*</a>,
                                            <a href="https://www.zyz.lol/app/aboutme/aboutme.html">Yizhou Zhao</a>,
                                            <a href="https://geng-haoran.github.io/">Haoran Geng</a>,
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="https://qywu.github.io/">Qingyang Wu</a>,
                                            <a href="https://wensi-ai.github.io/">Wensi Ai</a>,
                                            <a href="https://www.linkedin.com/in/josephziheng/">Ziheng Zhou</a>,
                                            <a href="http://web.cs.ucla.edu/~dt/">Demetri Terzopoulos</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
                                            <a href="https://buzz-beater.github.io/">Baoxiong Jia</a>,
                                            <a href="https://siyuanhuang.com/">Siyuan Huang</a> <br>
                                            <em> International Conference on Computer Vision (ICCV), 2023</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#arnold-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/ICCV2023_ARNOLD.pdf">Paper</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#arnold-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://arnold-benchmark.github.io/">Website</a>
                                        </div>
                                    </div>
                                    <div id="arnold-abs" class="collapse abstract">
                                        Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results indicate that current models for language-conditioned manipulations continue to experience significant challenges in novel goal-state generalizations, scene generalizations, and object generalizations. These findings highlight the need to develop new algorithms that address this gap and underscore the potential for further research in this area.
                                    </div>

                                    <div id="arnold-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@InProceedings{gong2023arnold,
    author    = {Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
    title     = {ARNOLD: A Benchmark for Language-Grounded Task Learning with Continuous States in Realistic 3D Scenes},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {20483-20495}
}</pre>
                                    </div>  
                                </li>


                                <!-- ScoutExploration -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/scoutExploration.jpg" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">In Situ Bidirectional Human-Robot Value Alignment</h4>
                                        <p class="detail">
                                            <a href="https://yuanluya.github.io/">Luyao Yuan*</a>,
                                            <strong>Xiaofeng Gao*</strong>, 
                                            <a href="https://zilongzheng.github.io/">Zilong Zheng*</a>,
                                            <a href="https://mjedmonds.com/">Mark Edmonds</a>,
                                            <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a>,
                                            <a href="https://cogsci.ucsd.edu/people/faculty/federico-rossano.html">Federico Rossano</a>,
                                            <a href="http://cvl.psych.ucla.edu/">Hongjing Lu</a>,
                                            <a href="https://yzhu.io/">Yixin Zhu</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a> <br>
                                            <em> Science Robotics, 2022</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#scoutExploration-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://www.science.org/doi/10.1126/scirobotics.abm4183">Paper</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#scoutExploration-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://doi.org/10.5068/D1XT3V">Code&Data</a>
                                        </div>
                                    </div>
                                    <div id="scoutExploration-abs" class="collapse abstract">
                                        A prerequisite for social coordination is bidirectional communication between teammates, each playing two roles simultaneously: as receptive listeners and expressive speakers. For robots working with humans in complex situations with multiple goals that differ in importance, failure to fulfill the expectation of either role could undermine group performance due to misalignment of values between humans and robots. Specifically, a robot needs to serve as an effective listener to infer human users’ intents from instructions and feedback and as an expressive speaker to explain its decision processes to users. Here, we investigate how to foster effective bidirectional human-robot communications in the context of value alignment—collaborative robots and users form an aligned understanding of the importance of possible task goals. We propose an explainable artificial intelligence (XAI) system in which a group of robots predicts users’ values by taking in situ feedback into consideration while communicating their decision processes to users through explanations. To learn from human feedback, our XAI system integrates a cooperative communication model for inferring human values associated with multiple desirable goals. To be interpretable to humans, the system simulates human mental dynamics and predicts optimal explanations using graphical models. We conducted psychological experiments to examine the core components of the proposed computational framework. Our results show that real-time human-robot mutual understanding in complex cooperative tasks is achievable with a learning model based on bidirectional communication. We believe that this interaction framework can shed light on bidirectional value alignment in communicative XAI systems and, more broadly, in future human-machine teaming systems.
                                    </div>

                                    <div id="scoutExploration-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{yuan2022in,
  title={In situ bidirectional human-robot value alignment},
  author={Yuan, Luyao and Gao, Xiaofeng and Zheng, Zilong and Edmonds, Mark and Wu, Ying Nian and Rossano, Federico and Lu, Hongjing and Zhu, Yixin and Zhu, Song-Chun},
  journal={Science Robotics},
  volume={7},
  number={68},
  year={2022},
  publisher={Science Robotics}
}</pre>
                                    </div>  
                                </li>

                                <!-- DialFRED -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/dialfred_intro.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following</h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            Qiaozi Gao,
                                            Ran Gong,
                                            Kaixiang Lin,
                                            Govind Thattai,
                                            Gaurav S. Sukhatme <br>
                                            <em> IEEE Robotics and Automation Letters (RA-L), 2022</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#dialfred-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/RAL2022_DialFRED.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#dialfred-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://github.com/xfgao/DialFRED">Code&Data</a>
                                        </div>
                                    </div>
                                    <div id="dialfred-abs" class="collapse abstract">
                                        Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53K task-relevant questions and answers and an oracle to answer questions. To solve DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents.
                                    </div>
                                    <div id="dialfred-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gao2022dialfred,
  title={DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following}, 
  author={Gao, Xiaofeng and Gao, Qiaozi and Gong, Ran and Lin, Kaixiang and Thattai, Govind and Sukhatme, Gaurav S.},
  journal={IEEE Robotics and Automation Letters}, 
  year={2022},
  volume={7},
  number={4},
  pages={10049-10056},
  doi={10.1109/LRA.2022.3193254}
}</pre>
                                    </div>  
                                </li>

                                <!-- HMI -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/simulator_third_person.jpg" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Effects of Augmented-Reality-Based Assisting Interfaces on Drivers' Object-Wise Situational Awareness in Highly Autonomous Vehicles</h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            Xingwei Wu,
                                            Samson Ho,
                                            Teruhisa Misu,
                                            Kumar Akash <br>
                                            <em> IEEE Intelligent Vehicles Symposium (IV), 2022</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#hmi-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/IV2022_SA.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#hmi-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://youtu.be/vMPdeoSn0CE">Talk</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/slides/IV22_SA.pdf">Slides</a>
                                        </div>
                                    </div>
                                    <div id="hmi-abs" class="collapse abstract">
                                        Although partially autonomous driving (AD) systems are already available in production vehicles, drivers are still required to maintain a sufficient level of situational awareness (SA) during driving. Previous studies have shown that providing information about the AD's capability using user interfaces can improve the driver's SA. However, displaying too much information increases the driver's workload and can distract or overwhelm the driver. Therefore, to design an efficient user interface (UI), it is necessary to understand its effect under different circumstances. In this paper, we focus on a UI based on augmented reality (AR), which can highlight potential hazards on the road. To understand the effect of highlighting on drivers' SA for objects with different types and locations under various traffic densities, we conducted an in-person experiment with 20 participants on a driving simulator. Our study results show that the effects of highlighting on drivers' SA varied by traffic densities, object locations and object types. We believe our study can provide guidance in selecting which object to highlight for the AR-based driver-assistance interface to optimize SA for drivers driving and monitoring partially autonomous vehicles. 
                                    </div>
                                    <div id="hmi-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@inproceedings{gao2022effects,
  title={Effects of Augmented-Reality-Based Assisting Interfaces on Drivers' Object-wise Situational Awareness in Highly Autonomous Vehicles}, 
  author={Gao, Xiaofeng and Wu, Xingwei and Ho, Samson and Misu, Teruhisa and Akash, Kumar},
  booktitle={2022 IEEE Intelligent Vehicles Symposium (IV)}, 
  pages={563-572},
  year={2022},
  organization={IEEE}
}</pre>

                                    </div>  
                                </li>

                                <!-- Capability Calibration -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/capcalib_intro.jpeg" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration</h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="https://yuanluya.github.io/">Luyao Yuan</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="http://cvl.psych.ucla.edu/">Hongjing Lu</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> IEEE Robotics and Automation Letters (RA-L), 2022</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#capcalib-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/RAL2022_CapCalib.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#capcalib-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/calib2022ral">Website</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://youtu.be/-dxtKtUt7Os">Talk</a>
                                        </div>
                                    </div>
                                    <div id="capcalib-abs" class="collapse abstract">
                                        Aligning humans' assessment of what a robot can do with its true capability is crucial for establishing a common ground between human and robot partners when they collaborate on a joint task. In this work, we propose an approach to calibrate humans' estimate of a robot's reachable workspace through a small number of demonstrations before collaboration. We develop a novel motion planning method, REMP (Reachability-Expressive Motion Planning), which jointly optimizes the physical cost and the expressiveness of robot motion to reveal the robot's motion capability to a human observer. Our experiments with human participants demonstrate that a short calibration using REMP can effectively bridge the gap between what a non-expert user thinks a robot can reach and the ground-truth. We show that this calibration procedure not only results in better user perception, but also promotes more efficient human-robot collaborations in a subsequent joint task.
                                    </div>
                                    <div id="capcalib-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gao2022show,
  title={Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration},
  author={Gao, Xiaofeng and Yuan, Luyao and Shu, Tianmin and Lu, Hongjing and Zhu, Song-Chun},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={2},
  pages={2644--2651},
  year={2022},
  publisher={IEEE}
}</pre>
                                    </div>  
                                </li>


                                <!-- TIP attention -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/attention_intro.jpg" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Predicting Task-Driven Attention via Integrating Bottom-Up Stimulus and Top-Down Guidance</h4>
                                        <p class="detail">
                                            Zhixiong Nan,
                                            Jingjing Jiang,
                                            <strong>Xiaofeng Gao</strong>, 
                                            Sanping Zhou, 
                                            Weiliang Zuo,
                                            Ping Wei,
                                            Nanning Zheng <br>
                                            <em> IEEE Transactions on Image Processing (TIP), 2021</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#attention-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/TIP2021_Attention.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#attention-cite">Cite</button>
                                        </div>
                                    </div>
                                    <div id="attention-abs" class="collapse abstract">
                                        Task-free attention has gained intensive interest in the computer vision community while relatively few works focus on task-driven attention (TDAttention). Thus this paper handles the problem of TDAttention prediction in daily scenarios where a human is doing a task. Motivated by the cognition mechanism that human attention allocation is jointly controlled by the top-down guidance and bottom-up stimulus, this paper proposes a cognitively-explanatory deep neural network model to predict TDAttention. Given an image sequence, bottom-up features, such as human pose and motion, are firstly extracted. At the same time, the coarse-grained task information and fine-grained task information are embedded as a top-down feature. The bottom-up features are then fused with the top-down feature to guide the model to predict TDAttention. Two public datasets are re-annotated to make them qualified for TDAttention prediction, and our model is widely compared with other models on the two datasets. In addition, some ablation studies are conducted to evaluate the individual modules in our model. Experiment results demonstrate the effectiveness of our model.
                                    </div>
                                    <div id="attention-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{nan2021predicting,
  title={Predicting Task-Driven Attention via Integrating Bottom-Up Stimulus and Top-Down Guidance},
  author={Nan, Zhixiong and Jiang, Jingjing and Gao, Xiaofeng and Zhou, Sanping and Zuo, Weiliang and Wei, Ping and Zheng, Nanning},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={8293--8305},
  year={2021},
  publisher={IEEE}
}</pre>
                                    </div>  
                                </li>

                                <!-- XCooking -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/map_final.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks</h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao*</strong>, 
                                            <a href="https://nikepupu.github.io/">Ran Gong*</a>,
                                            <a href="https://yizhouzhao.github.io/">Yizhou Zhao</a>,
                                            <a href="https://shuwang0712.github.io/">Shu Wang</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> IEEE International Conference on Robot & Human Interactive Communication (RO-MAN), 2020 </em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#XCooking-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/ROMAN2020_XCooking.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#XCooking-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/xCookingWeb/">Website</a> 
                                        <a class="btn btn-warning btn-xs" role="button" href="https://youtu.be/Q8jmEQ6JqQ0">Talk</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/slides/ROMAN20_collabCooking.pdf">Slides</a>
                                        </div>
                                    </div>
                                    <div id="XCooking-abs" class="collapse abstract">
                                        Human collaborators can effectively communicate with their partners to finish a common task by inferring each other's mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators' mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user's mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot.
                                    </div>
                                    <div id="XCooking-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@inproceedings{gao2020joint,
  title={Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks},
  author={Gao, Xiaofeng and Gong, Ran and Zhao, Yizhou and Wang, Shu and Shu, Tianmin and Zhu, Song-Chun},
  booktitle={2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  pages={1119--1126},
  year={2020},
  organization={IEEE}
}</pre>

                                    </div>  
                                </li>

                                <!-- VRKitchen -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/dish_fluent_small.gif" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">VRKitchen: an Interactive 3D Environment for Learning Real Life Cooking Tasks</h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="https://nikepupu.github.io/">Ran Gong</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="https://xuxie1031.github.io/">Xu Xie</a>,
                                            <a href="https://shuwang0712.github.io/">Shu Wang</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> ICML workshop on Reinforcement Learning for Real Life, 2019 </em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#kitchen-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/Arxiv2019_VRKitchen.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#kitchen-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://sites.google.com/view/vr-kitchen/">Website</a> 
                                        </div>
                                    </div>
                                    <div id="kitchen-abs" class="collapse abstract">
                                        One of the main challenges of applying reinforcement learning to real world applications is the lack of realistic and standardized environments for training and testing AI agents. In this work, we design and implement a virtual reality (VR) system, VRKitchen, with integrated functions which i) enable embodied agents to perform real life cooking tasks involving a wide range of object manipulations and state changes, and ii) allow human teachers to provide demonstrations for training agents. We also provide standardized evaluation benchmarks and data collection tools to facilitate a broad use in research on learning real life tasks. Video demos, code, and data will be available on the project website: sites.google.com/view/vr-kitchen.
                                    </div>
                                    <div id="kitchen-cite" class="collapse abstract">
                                        <pre class="citation" align="left">
@article{gao2019vrkitchen,
  title={Vrkitchen: an interactive 3d virtual environment for task-oriented learning},
  author={Gao, Xiaofeng and Gong, Ran and Shu, Tianmin and Xie, Xu and Wang, Shu and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:1903.05757},
  year={2019}
}</pre>

                                    </div>  
                                </li>

                                <!-- Human-Robot Social Interaction -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/hri_intro.png" class="img-responsive img-fluid" width="150">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Learning Social Affordance Grammar from Videos: 
                                        	Transferring Human Interactions to Human-Robot Interactions </h4>
                                        <p class="detail">
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="http://michaelryoo.com/">Michael S. Ryoo</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2017
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#social_interaction-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/ICRA2017_SocialAffordance.pdf">PDF</a>
                                        <button class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#social_interaction-cite">Cite</button>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://www.tshu.io/SocialAffordanceGrammar/index.html">Website</a> 
                                        </div>
                                    </div>
                                    <div id="social_interaction-abs" class="collapse abstract">
                                        In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines.
                                    </div>
                                    <div id="social_interaction-cite" class="collapse abstract">
                                    	<pre class="citation" align="left">
@inproceedings{shu2017learning,
  title={Learning social affordance grammar from videos: Transferring human interactions to human-robot interactions},
  author={Shu, Tianmin and Gao, Xiaofeng and Ryoo, Michael S and Zhu, Song-Chun},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={1669--1676},
  year={2017},
  organization={IEEE}
}</pre>

                                    </div>	
                                </li>


                            </ul>
                        </div>
                </div>
            </div>
        </div>
    </section>
    <hr>

    <footer class="footer">
        <div class="container">
            <p class="text-muted">© 2023 All rights reserved. Developed by Xiaofeng Gao.</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="./css/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./css/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="./css/jquery.easing.min.js"></script>
    <script src="./css/scrolling-nav.js"></script>




</body></html>
